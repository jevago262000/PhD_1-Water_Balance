{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b67f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy, pandas as pd, os, multiprocessing as mp, psutil, time\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "from otherfunctions import folders_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input datasets\n",
    "root_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\"\n",
    "tam_out_dir = r\"Z:\\PhD_Datasets&Analysis\\Outputs\\T&M_WBM\"\n",
    "tc_ds = root_folder + \"\\\\TerraClimate\"\n",
    "out_geotiff = tc_ds + \"\\\\GeoTIFF\"\n",
    "bands_gee = [\"pr\", \"pet\", \"ro\"] # band names in GEE - for comparison with GEE TerraClimate dataset\n",
    "tc_vars = [\"ppt\", \"pet\", \"q\"] # variable names according to TerraClimate\n",
    "serial_id = 'grdcno_int'\n",
    "\n",
    "# Set arcpy environment variables\n",
    "env.overwriteOutput = True\n",
    "arcpy.CheckOutExtension(\"spatial\")\n",
    "# env.cellSize = \"MINOF\" # Avoided to prevent huge files\n",
    "env.cellSize = out_geotiff + \"\\\\ppt_2023_1.tif\" # Use TerraClimate resolution as reference for cell size\n",
    "env.workspace = r\"Z:\\PhD_Datasets&Analysis\\_ProcessingCache\"\n",
    "env.outputCoordinateSystem = arcpy.SpatialReference(\"WGS 1984\") # WGS 1984 (4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d41050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current environment's spatial reference\n",
    "spatial_ref = env.outputCoordinateSystem\n",
    "\n",
    "# Check if a spatial reference is set\n",
    "if spatial_ref:\n",
    "    print(f\"Spatial Reference Name: {spatial_ref.name}\")\n",
    "    print(f\"Spatial Reference WKID: {spatial_ref.factoryCode}\")\n",
    "else:\n",
    "    print(\"No spatial reference is set in the current environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Shapefile with the processed drainage areas\n",
    "drain_areas = root_folder + \"\\\\Streamflow_Sts_Drainage_Areas\\GRDC_Watersheds\\CSS-WATERSHEDS_FINAL_SELECTION.shp\"\n",
    "\n",
    "# Create a feature layer object\n",
    "arcpy.MakeFeatureLayer_management(drain_areas, \"drain_areas_lyr\")\n",
    "\n",
    "# Initialize an empty list to store the station IDs\n",
    "sts_ids = []\n",
    "\n",
    "# Use a SearchCursor to iterate through the rows of the feature layer\n",
    "with arcpy.da.SearchCursor(\"drain_areas_lyr\", [serial_id]) as cursor:\n",
    "    for row in cursor:\n",
    "        sts_ids.append(row[0])\n",
    "\n",
    "sts_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Starting values for the water balance model - T&M\n",
    "######################################################\n",
    "\n",
    "# Initial variables\n",
    "years = list(range(1958, 1967 + 1)) # Years to process. This line can be used to execute this code for specific years in multiple runs.\n",
    "months = range(1, 12 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for other variables of tam model\n",
    "wyield_dir = tam_out_dir + '\\\\wyield'\n",
    "folders_exist([wyield_dir])\n",
    "\n",
    "# Folder with baseflow rasters resulting from the model\n",
    "bflow_dir = tam_out_dir + '\\\\bflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable better logging from multiprocessing\n",
    "# This must be set before creating any processes\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "def zonal_stastics_iteratively(year):\n",
    "    \"\"\"\n",
    "    Function to calculate zonal statistics iteratively for each station ID.\n",
    "    \"\"\"\n",
    "    # Improved logging with timestamps\n",
    "    process_id = os.getpid()\n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] Process {process_id} STARTED - Calculating zonal statistics for year {year}\")\n",
    "    \n",
    "    try:\n",
    "        # Check out the spatial analyst extension in each process\n",
    "        if arcpy.CheckExtension(\"Spatial\") == \"Available\":\n",
    "            arcpy.CheckOutExtension(\"Spatial\")\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - Spatial Analyst checked out successfully\")\n",
    "        else:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - ERROR: Spatial Analyst extension not available!\")\n",
    "            return f\"ERROR: Year {year} - Spatial Analyst not available\"\n",
    "\n",
    "        sts_flows_sim = pd.DataFrame(columns=[serial_id, \"YEAR\", \"MONTH\", \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"])  \n",
    "\n",
    "        # Create a feature layer for this process with a unique name\n",
    "        layer_name = f\"drain_areas_lyr_{process_id}\"\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - Creating feature layer {layer_name}\")\n",
    "        arcpy.MakeFeatureLayer_management(drain_areas, layer_name)\n",
    "\n",
    "        total_stations = len(sts_ids)\n",
    "        for i, st in enumerate(sts_ids):\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - Year {year} - Station {st} ({i+1}/{total_stations})\")\n",
    "            \n",
    "            # Select the current station ID in the feature layer\n",
    "            arcpy.SelectLayerByAttribute_management(layer_name, \"NEW_SELECTION\", f\"{serial_id} = {st}\")\n",
    "\n",
    "            for m, month in enumerate(months):\n",
    "                if m % 4 == 0:  # Only log every few months to avoid excessive output\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - Year {year} - Station {st} - Processing month {month}\")\n",
    "                \n",
    "                wyield = os.path.join(wyield_dir, f\"wyield_{year}_{month}.tif\")\n",
    "                out_table = f\"in_memory\\\\zonal_wyield_{st}_{year}_{month}_{process_id}\"\n",
    "                \n",
    "                # Verify input file exists\n",
    "                if not arcpy.Exists(wyield):\n",
    "                    print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - WARNING: {wyield} does not exist, skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Run zonal statistics\n",
    "                arcpy.sa.ZonalStatisticsAsTable(layer_name, serial_id, wyield, out_table, \"DATA\", \"ALL\")\n",
    "\n",
    "                # Convert the output table to a NumPy array\n",
    "                array = arcpy.da.TableToNumPyArray(out_table, [serial_id, \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"])\n",
    "\n",
    "                # Convert the NumPy array to a pandas DataFrame\n",
    "                df_sim = pd.DataFrame(array)\n",
    "\n",
    "                df_sim[\"YEAR\"] = year \n",
    "                df_sim[\"MONTH\"] = month\n",
    "                df_sim = df_sim[[serial_id, \"YEAR\", \"MONTH\", \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"]]\n",
    "\n",
    "                sts_flows_sim = pd.concat([sts_flows_sim, df_sim], ignore_index=True)\n",
    "\n",
    "                # Clean up the temp table\n",
    "                arcpy.Delete_management(out_table)\n",
    "\n",
    "        # Clean up the feature layer\n",
    "        arcpy.Delete_management(layer_name)\n",
    "        \n",
    "        # Save the results to a CSV file for this year\n",
    "        csv_path = os.path.join(wyield_dir, f\"wyield_zonal_statistics_{year}.csv\")\n",
    "        sts_flows_sim.to_csv(csv_path, index=False)\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - COMPLETED - Year {year} - Saved results to {csv_path}\")\n",
    "        \n",
    "        # Check in the extension\n",
    "        arcpy.CheckInExtension(\"Spatial\")\n",
    "        \n",
    "        return f\"SUCCESS: Year {year}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Process {process_id} - ERROR processing year {year}: {str(e)}\")\n",
    "        # Try to check in the extension in case of error\n",
    "        try:\n",
    "            arcpy.CheckInExtension(\"Spatial\")\n",
    "        except:\n",
    "            pass\n",
    "        return f\"ERROR: Year {year} - {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_processing(years_to_process):\n",
    "    \"\"\"\n",
    "    Run the zonal statistics calculations in parallel for multiple years,\n",
    "    with safeguards to prevent machine overload.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] Starting parallel processing setup...\")\n",
    "    \n",
    "    # Determine the number of cores to use\n",
    "    total_cores = mp.cpu_count()\n",
    "    max_cores = max(1, int(total_cores * 0.75))\n",
    "    \n",
    "    # Further limit cores based on available memory\n",
    "    memory_per_process_gb = 4  # Estimate 4GB per process\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024 * 1024 * 1024)\n",
    "    memory_limited_cores = max(1, int(available_memory_gb / memory_per_process_gb))\n",
    "    \n",
    "    # Use the more conservative limit\n",
    "    num_processes = min(max_cores, memory_limited_cores, len(years_to_process))\n",
    "    \n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] System has {total_cores} cores, using {num_processes} for parallel processing\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Available memory: {available_memory_gb:.2f} GB, estimated usage: {memory_per_process_gb * num_processes:.2f} GB\")\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Processing {len(years_to_process)} years: {years_to_process}\")\n",
    "    \n",
    "    # Create a pool of worker processes\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] Creating process pool with {num_processes} workers...\")\n",
    "    \n",
    "    # Start processing each year with a small delay between processes\n",
    "    # to avoid resource contention at startup\n",
    "    results = []\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        for year in years_to_process:\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] Submitting year {year} to the process pool...\")\n",
    "            # Use apply_async instead of map to get immediate feedback\n",
    "            results.append(pool.apply_async(zonal_stastics_iteratively, (year,)))\n",
    "            time.sleep(2)  # Small delay between process starts\n",
    "        \n",
    "        # Wait for all processes to complete and collect results\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] Waiting for all processes to complete...\")\n",
    "        all_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            try:\n",
    "                res = result.get(timeout=7200)  # 2-hour timeout per process\n",
    "                all_results.append(res)\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Process {i+1}/{len(results)} completed: {res}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{time.strftime('%H:%M:%S')}] Process {i+1}/{len(results)} failed: {str(e)}\")\n",
    "                all_results.append(f\"FAILED: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] All processing completed with results: {all_results}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18202884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n############################################################')\n",
    "print('\\t\\tINITIAL VARIABLES')\n",
    "print(f'\\tPeriod to be executed: {years[0]}-{years[-1]}')\n",
    "print('############################################################')\n",
    "\n",
    "try:\n",
    "    # Run the parallel processing\n",
    "    start_time = time.time()\n",
    "    run_parallel_processing(years)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] Total execution time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[{time.strftime('%H:%M:%S')}] ERROR in main process: {str(e)}\")\n",
    "finally:\n",
    "    # Make sure to check in extensions in the main process\n",
    "    try:\n",
    "        arcpy.CheckInExtension(\"spatial\")\n",
    "        arcpy.ClearEnvironment(\"workspace\")\n",
    "        print(f\"\\n[{time.strftime('%H:%M:%S')}] Cleaned up resources in main process\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"\\nDONE!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
