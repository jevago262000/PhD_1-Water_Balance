{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b67f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy, pandas as pd, os, multiprocessing as mp, psutil\n",
    "from arcpy import env\n",
    "from arcpy.sa import *\n",
    "from otherfunctions import folders_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input datasets\n",
    "root_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\"\n",
    "tam_out_dir = r\"Z:\\PhD_Datasets&Analysis\\Outputs\\T&M_WBM\"\n",
    "tc_ds = root_folder + \"\\\\TerraClimate\"\n",
    "out_geotiff = tc_ds + \"\\\\GeoTIFF\"\n",
    "bands_gee = [\"pr\", \"pet\", \"ro\"] # band names in GEE - for comparison with GEE TerraClimate dataset\n",
    "tc_vars = [\"ppt\", \"pet\", \"q\"] # variable names according to TerraClimate\n",
    "serial_id = 'grdcno_int'\n",
    "\n",
    "# Set arcpy environment variables\n",
    "env.overwriteOutput = True\n",
    "arcpy.CheckOutExtension(\"spatial\")\n",
    "# env.cellSize = \"MINOF\" # Avoided to prevent huge files\n",
    "env.cellSize = out_geotiff + \"\\\\ppt_2023_1.tif\" # Use TerraClimate resolution as reference for cell size\n",
    "env.workspace = r\"Z:\\PhD_Datasets&Analysis\\_ProcessingCache\"\n",
    "env.outputCoordinateSystem = arcpy.SpatialReference(\"WGS 1984\") # WGS 1984 (4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d41050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current environment's spatial reference\n",
    "spatial_ref = env.outputCoordinateSystem\n",
    "\n",
    "# Check if a spatial reference is set\n",
    "if spatial_ref:\n",
    "    print(f\"Spatial Reference Name: {spatial_ref.name}\")\n",
    "    print(f\"Spatial Reference WKID: {spatial_ref.factoryCode}\")\n",
    "else:\n",
    "    print(\"No spatial reference is set in the current environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Shapefile with the processed drainage areas\n",
    "drain_areas = root_folder + \"\\\\Streamflow_Sts_Drainage_Areas\\GRDC_Watersheds\\CSS-WATERSHEDS_FINAL_SELECTION.shp\"\n",
    "\n",
    "# Create a feature layer object\n",
    "arcpy.MakeFeatureLayer_management(drain_areas, \"drain_areas_lyr\")\n",
    "\n",
    "# Initialize an empty list to store the station IDs\n",
    "sts_ids = []\n",
    "\n",
    "# Use a SearchCursor to iterate through the rows of the feature layer\n",
    "with arcpy.da.SearchCursor(\"drain_areas_lyr\", [serial_id]) as cursor:\n",
    "    for row in cursor:\n",
    "        sts_ids.append(row[0])\n",
    "\n",
    "sts_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "### Starting values for the water balance model - T&M\n",
    "######################################################\n",
    "\n",
    "# Initial variables\n",
    "years = range(1958, 1967 + 1) # Years to process. This line can be used to execute this code for specific years in multiple runs.\n",
    "months = range(1, 12 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for other variables of tam model\n",
    "wyield_dir = tam_out_dir + '\\\\wyield'\n",
    "folders_exist([wyield_dir])\n",
    "\n",
    "# Folder with baseflow rasters resulting from the model\n",
    "bflow_dir = tam_out_dir + '\\\\bflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stastics_iteratively(year):\n",
    "    \"\"\"\n",
    "    Function to calculate zonal statistics iteratively for each station ID.\n",
    "    \"\"\"\n",
    "    print(f\"\\t[Process {os.getpid()}] Calculating zonal statistics of water yield for year {year}......\")\n",
    "\n",
    "    sts_flows_sim = pd.DataFrame(columns=[serial_id, \"YEAR\", \"MONTH\", \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"])  \n",
    "\n",
    "    # Create a feature layer for this process\n",
    "    arcpy.MakeFeatureLayer_management(drain_areas, f\"drain_areas_lyr_{os.getpid()}\")\n",
    "\n",
    "    for st in sts_ids:\n",
    "        print(f\"\\t\\t[Process {os.getpid()}] Station ID: {st}\")\n",
    "        \n",
    "        # Select the current station ID in the feature layer\n",
    "        arcpy.SelectLayerByAttribute_management(f\"drain_areas_lyr_{os.getpid()}\", \"NEW_SELECTION\", f\"{serial_id} = {st}\")\n",
    "\n",
    "        for month in months:\n",
    "            wyield = wyield_dir + \"\\\\wyield_\" + str(year) + \"_\" + str(month) + \".tif\"\n",
    "            out_table = f\"zonal_wyield_{st}_{year}_{month}_{os.getpid()}.dbf\"\n",
    "\n",
    "            arcpy.sa.ZonalStatisticsAsTable(f\"drain_areas_lyr_{os.getpid()}\", serial_id, wyield, out_table, \"DATA\", \"ALL\")\n",
    "\n",
    "            # Convert the output table to a NumPy array\n",
    "            array = arcpy.da.TableToNumPyArray(out_table, [serial_id, \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"])\n",
    "\n",
    "            # Convert the NumPy array to a pandas DataFrame\n",
    "            df_sim = pd.DataFrame(array)\n",
    "\n",
    "            df_sim[\"YEAR\"] = year # Assign the year of simulation\n",
    "            df_sim[\"MONTH\"] = month # Assign the month of simulation\n",
    "            df_sim = df_sim[[serial_id, \"YEAR\", \"MONTH\", \"COUNT\", \"AREA\", \"MIN\", \"MAX\", \"RANGE\", \"MEAN\", \"STD\", \"SUM\", \"MEDIAN\", \"PCT90\"]] # Reorder columns\n",
    "\n",
    "            sts_flows_sim = pd.concat([sts_flows_sim, df_sim], ignore_index=True) # Concat all simulated stream flow station values\n",
    "\n",
    "            arcpy.Delete_management(out_table) # Delete the output table to save space\n",
    "\n",
    "    # Clean up the feature layer\n",
    "    arcpy.Delete_management(f\"drain_areas_lyr_{os.getpid()}\")\n",
    "    \n",
    "    # Save the results to a CSV file for this year\n",
    "    csv_path = wyield_dir + \"\\\\wyield_zonal_statistics_\" + str(year) + \".csv\"\n",
    "    sts_flows_sim.to_csv(csv_path, index=False)\n",
    "    print(f\"\\t[Process {os.getpid()}] Saved results for year {year} to {csv_path}\")\n",
    "    \n",
    "    return year  # Just return the year to confirm completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_worker():\n",
    "    \"\"\"Initialize worker process with ArcPy license checkout\"\"\"\n",
    "    arcpy.CheckOutExtension(\"spatial\")\n",
    "    print(f\"Worker process {os.getpid()} initialized with Spatial Analyst extension\")\n",
    "\n",
    "def run_parallel_processing(years_to_process):\n",
    "    \"\"\"\n",
    "    Run the zonal statistics calculations in parallel for multiple years,\n",
    "    with safeguards to prevent machine overload.\n",
    "    \"\"\"\n",
    "    # Determine the number of cores to use\n",
    "    # Use at most 75% of available cores to prevent overloading the system\n",
    "    total_cores = mp.cpu_count()\n",
    "    max_cores = max(1, int(total_cores * 0.75))\n",
    "    \n",
    "    # Further limit cores based on available memory\n",
    "    # Estimate 4GB per process (adjust this based on your observation)\n",
    "    memory_per_process_gb = 4\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024 * 1024 * 1024)\n",
    "    memory_limited_cores = max(1, int(available_memory_gb / memory_per_process_gb))\n",
    "    \n",
    "    # Use the more conservative limit\n",
    "    num_processes = min(max_cores, memory_limited_cores, len(years_to_process))\n",
    "    \n",
    "    print(f\"\\nSystem has {total_cores} cores, using {num_processes} for parallel processing\")\n",
    "    print(f\"Available memory: {available_memory_gb:.2f} GB, estimated usage: {memory_per_process_gb * num_processes:.2f} GB\")\n",
    "    \n",
    "    # Create a pool of worker processes\n",
    "    with mp.Pool(processes=num_processes, initializer=init_worker) as pool:\n",
    "        # Map the processing function to each year\n",
    "        results = pool.map(zonal_stastics_iteratively, years_to_process)\n",
    "    \n",
    "    print(f\"\\nCompleted processing for years: {results}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18202884",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n############################################################')\n",
    "print('\\t\\tINITIAL VARIABLES')\n",
    "print('\\tPeriod to be executed: ' + str(years[0]) + '-' + str(years[-1]))\n",
    "print('############################################################')\n",
    "\n",
    "# Run the parallel processing\n",
    "run_parallel_processing(years)\n",
    "\n",
    "# Check in the extension when all processing is done\n",
    "arcpy.CheckInExtension(\"spatial\")\n",
    "\n",
    "# Clear the workspace environment\n",
    "arcpy.ClearEnvironment(\"workspace\")\n",
    "\n",
    "print(\"\\nDONE!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
