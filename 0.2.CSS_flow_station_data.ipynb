{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, glob, numpy as np\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily processing\n",
    "daily_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\\Streamflow_Stations\\Climate_Sensitive_Stations-GRDC\\2025-02-13_17-18_Daily\"\n",
    "monthly_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\\Streamflow_Stations\\Climate_Sensitive_Stations-GRDC\\2025-02-13_17-18_Monthly\"\n",
    "watersheds_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\\Streamflow_Sts_Drainage_Areas\\GRDC_Watersheds\"\n",
    "ext = \"*.txt\"\n",
    "\n",
    "# TerraClimate available period\n",
    "terra_st_yr = 1958\n",
    "terra_ed_yr = 2023\n",
    "sdate = date(terra_st_yr, 1, 1) # start date\n",
    "edate = date(terra_ed_yr, 12, 31) # end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_dates = [(sdate + timedelta(days=i)).strftime('%Y-%m-%d') for i in range((edate - sdate).days + 1)]\n",
    "\n",
    "# Create a DF whose index corresponds to the TerraClimate dates\n",
    "daily_df_sts = pd.DataFrame({\"YYYY-MM-DD\": period_dates}).set_index(\"YYYY-MM-DD\")\n",
    "daily_df_sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily processing\n",
    "archivos = glob.glob(daily_folder + \"//\" + ext)\n",
    "\n",
    "for archivo in archivos:\n",
    "\n",
    "    sts_dict = {}\n",
    "    file_name = archivo.split(\"\\\\\")[-1]\n",
    "    print(\"Reading file: \" + file_name)\n",
    "\n",
    "    id_station = file_name.split(\"_\")[0]\n",
    "    data_matrix = []\n",
    "\n",
    "    # Specify encoding explicitly\n",
    "    with open(archivo, 'r', encoding='ISO-8859-1') as inFile:    \n",
    "        data_matrix = inFile.readlines()[37:] # starting data line in the file\n",
    "\n",
    "    date_array = []\n",
    "    value_array = []\n",
    "\n",
    "    for data in data_matrix:\n",
    "        line = data.split(\";\")\n",
    "        line_date = line[0]\n",
    "\n",
    "        try:\n",
    "            line_value = float(line[-1])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping invalid value in {archivo}: {line[-1]}\")\n",
    "            continue\n",
    "\n",
    "        if int(line_value) == -999: # This avoids saving no-data values (-999)\n",
    "            line_value = np.nan\n",
    "\n",
    "        date_array.append(line_date)\n",
    "        value_array.append(line_value)\n",
    "\n",
    "    if len(value_array) == 0: # This avoids saving files with no station data\n",
    "        print(f\"Skipping station {id_station} due to no data\")\n",
    "        continue\n",
    "\n",
    "    sts_dict[\"YYYY-MM-DD\"] = date_array\n",
    "    sts_dict[id_station] = value_array\n",
    "\n",
    "    # Create a DF whose index corresponds to the TerraClimate dates\n",
    "    temp_df = pd.DataFrame(sts_dict).set_index(\"YYYY-MM-DD\")\n",
    "    daily_df_sts = daily_df_sts.join(temp_df) # left join on index\n",
    "\n",
    "# Drop columns with all NaN values\n",
    "daily_df_cleaned = daily_df_sts.dropna(axis=1, how='all')\n",
    "#daily_df_cleaned.to_csv(daily_folder + \"\\_DataFrames\\Joined_Daily_Sts_DFs.csv\")\n",
    "daily_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(terra_st_yr, terra_ed_yr + 1)\n",
    "months = range(1, 12 + 1)\n",
    "yr_mth= [str(a)+ \"-\" + str(b).zfill(2) for a in years for b in months]\n",
    "\n",
    "# Create a DF whose index corresponds to the TerraClimate monthly dates\n",
    "monthly_df_sts = pd.DataFrame({\"YYYY-MM\": yr_mth}).set_index(\"YYYY-MM\")\n",
    "monthly_df_sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file with UTF-8 encoding that contains information on the CSS-related watersheds. This file contains only 1,236 records as 9 CSS stations did not have delineated watersheds provided by GRDC\n",
    "drain_areas_df = pd.read_csv(watersheds_folder + \"\\\\CSS-GRDC_Watersheds.csv\") # grdc_no == station_no, area == CATCHMENT_SIZE \n",
    "drain_areas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures working only with stations whose watersheds are bigger than three TerraClimate pixels, considering a pixel size of 4 km. Therefore, 4 x 4 x 3 = 48\n",
    "# This facilitates zonal statistics, which are executed after the water balance to extract values from the resulting surfaces for the ultimate watersheds. \n",
    "filtered_drain_areas_df = drain_areas_df[drain_areas_df[\"area\"] > 48]\n",
    "filtered_drain_areas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_ids = filtered_drain_areas_df[\"grdc_no\"].astype(int).to_list()\n",
    "print(sts_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly processing\n",
    "archivos = glob.glob(monthly_folder + \"//\" + ext)\n",
    "\n",
    "for archivo in archivos:\n",
    "\n",
    "    sts_dict = {}\n",
    "    file_name = archivo.split(\"\\\\\")[-1]\n",
    "    print(\"Reading file: \" + file_name)\n",
    "\n",
    "    id_station = file_name.split(\"_\")[0]\n",
    "\n",
    "    if int(id_station) not in sts_ids: # This avoids reading files of stations that do not have their respective watersheds\n",
    "        continue\n",
    "\n",
    "    data_matrix = []\n",
    "\n",
    "    # Specify encoding explicitly\n",
    "    with open(archivo, 'r', encoding='ISO-8859-1') as inFile:    \n",
    "        data_matrix = inFile.readlines()[39:] # starting data line in the file\n",
    "\n",
    "    date_array = []\n",
    "    value_array = []\n",
    "\n",
    "    for data in data_matrix:\n",
    "        line = data.split(\";\")\n",
    "        line_date = '-'.join(line[0].split(\"-\")[0:2])\n",
    "\n",
    "        try:\n",
    "            line_value = float(line[-2])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping invalid value in {archivo}: {line[-2]}\")\n",
    "            continue\n",
    "\n",
    "        if int(line_value) == -999: # This avoids saving no-data values (-999)\n",
    "            line_value = np.nan\n",
    "\n",
    "        date_array.append(line_date)\n",
    "        value_array.append(line_value)\n",
    "\n",
    "    if len(value_array) == 0: # This avoids saving files with no station data\n",
    "        print(f\"Skipping station {id_station} due to no data\")\n",
    "        continue\n",
    "\n",
    "    sts_dict[\"YYYY-MM\"] = date_array\n",
    "    sts_dict[id_station] = value_array\n",
    "\n",
    "    # Create a DF whose index corresponds to the TerraClimate monthly dates\n",
    "    temp_df = pd.DataFrame(sts_dict).set_index(\"YYYY-MM\")\n",
    "    monthly_df_sts = monthly_df_sts.join(temp_df) # left join on index\n",
    "\n",
    "# Drop columns with all NaN values\n",
    "monthly_df_cleaned = monthly_df_sts.dropna(axis=1, how='all')\n",
    "#monthly_df_cleaned.to_csv(daily_folder + \"\\_DataFrames\\Joined_Monthly_Sts_DFs.csv\")\n",
    "monthly_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify stations with at least 30 complete water-years (Oct-Sep) in a DataFrame where:\n",
    "\n",
    "- The index is in YYYY-MM format.\n",
    "- The columns are stations with their monthly multiannual values.\n",
    "\n",
    "1. Convert the index to a DateTime format\n",
    "Since the index is in YYYY-MM format, convert it to a proper datetime format for easier filtering and resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_monthly_df_cleaned = monthly_df_cleaned.copy()\n",
    "reindexed_monthly_df_cleaned.index = pd.to_datetime(reindexed_monthly_df_cleaned.index, format='%Y-%m')\n",
    "reindexed_monthly_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define Water Years (Oct-Sep)\n",
    "The water year starts in October and ends in September of the following year. You can define a water-year label as the year of the September within that water year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexed_monthly_df_cleaned['water_year'] = reindexed_monthly_df_cleaned.index.to_series().apply(lambda x: x.year if x.month < 10 else x.year + 1)\n",
    "reindexed_monthly_df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Count Complete Water Years for Each Station\n",
    "Now, we group by water year and count the number of non-null monthly values per station. A complete water year must have 12 valid values for a given station.\n",
    "\n",
    "- This method ensures that:\n",
    "\n",
    "    - Only stations with 30+ water years (each with all 12 months) are selected.\n",
    "    \n",
    "    - The approach is flexible for datasets with missing months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count valid months per water year per station\n",
    "valid_months_per_wy = reindexed_monthly_df_cleaned.groupby('water_year').count()\n",
    "\n",
    "# Identify stations with at least 30 complete water years. This guarantees that we are considering the same approach of TerraClimate authors for results validation\n",
    "stations_with_30_wy = (valid_months_per_wy == 12).sum(axis=0) >= 30\n",
    "selected_stations = stations_with_30_wy[stations_with_30_wy].index.tolist()\n",
    "print(selected_stations)  # List of stations with at least 30 complete water years\n",
    "print(\"The final amount of stations with at least 30 complete water years is\", len(selected_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final dataframes with the stations to be considered\n",
    "final_daily_df = daily_df_cleaned[selected_stations]\n",
    "final_daily_df.to_csv(daily_folder + \"\\_DataFrames\\Joined_Daily_Sts_DFs.csv\")\n",
    "\n",
    "final_monthly_df = monthly_df_cleaned[selected_stations]\n",
    "final_monthly_df.to_csv(monthly_folder + \"\\_DataFrames\\Joined_Monthly_Sts_DFs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_daily_df = pd.read_csv_csv(daily_folder + \"\\_DataFrames\\Joined_Daily_Sts_DFs.csv\", index_col=\"YYYY-MM\")\n",
    "final_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_monthly_df = pd.read_csv_csv(monthly_folder + \"\\_DataFrames\\Joined_Monthly_Sts_DFs.csv\", index_col=\"YYYY-MM\")\n",
    "final_monthly_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
