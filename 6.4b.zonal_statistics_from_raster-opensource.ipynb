{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b67f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterstats import zonal_stats\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not installed, run this cell. It is needed for watershed delineation\n",
    "#!pip install rasterstats\n",
    "#!pip show rasterstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b428b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "root_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\"\n",
    "css_folder = root_folder + \"\\\\Streamflow_Sts_Drainage_Areas\\GRDC_Watersheds\"\n",
    "tam_out_dir = r\"Z:\\PhD_Datasets&Analysis\\Outputs\\T&M_WBM\"\n",
    "tc_ds = root_folder + \"\\\\TerraClimate\"\n",
    "out_geotiff = tc_ds + \"\\\\GeoTIFF\"\n",
    "serial_id = 'grdcno_int'\n",
    "tc_vars = [\"ppt\", \"pet\", \"q\"] # variable names according to TerraClimate\n",
    "wb_var = 'wyield2' # Change this to the variable you want to process, e.g., 'wyield' for water yield\n",
    "sql_field = \"has_monthl\" # Field to filter rows with monthly k recessions\n",
    "#sql_field = \"has_daily_\" # Field to filter rows with daily k recessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61bd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing parameters\n",
    "#years = range(1958, 2023 + 1)\n",
    "years = range(1970, 2023 + 1)\n",
    "months = range(1, 12 + 1)\n",
    "n_processes = min(cpu_count() - 1, 50)  # Use available cores minus 1, max 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed0cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "processing_dir = os.path.join(tam_out_dir, wb_var)\n",
    "if wb_var in [\"bflow2\", \"wyield2\"]:\n",
    "    wb_var_clean = wb_var[:-1]  # Remove the last character '2'\n",
    "else:\n",
    "    wb_var_clean = wb_var\n",
    "\n",
    "# Note: raster_dir not needed anymore since we're using the vector layer directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d58417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_station_polygons():\n",
    "    \"\"\"\n",
    "    Load station drainage area polygons from the existing vector layer.\n",
    "    Returns a dictionary with station IDs as keys and geometries as values.\n",
    "    \"\"\"\n",
    "    print(\"Loading station drainage areas from vector layer...\")\n",
    "    station_polygons = {}\n",
    "    \n",
    "    # Drainage areas vector layer\n",
    "    drain_areas_file = os.path.join(css_folder, \"CSS-WATERSHEDS-MERGE_FINAL_SELECTION.shp\")  # Adjust file name as needed\n",
    "\n",
    "    if not drain_areas_file:\n",
    "        raise FileNotFoundError(f\"Could not find drainage areas vector file in {css_folder}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading from: {drain_areas_file}\")\n",
    "        \n",
    "        # Load the drainage areas vector layer\n",
    "        gdf = gpd.read_file(drain_areas_file)\n",
    "        \n",
    "        print(f\"Loaded vector layer with {len(gdf)} features\")\n",
    "        print(f\"Available columns: {list(gdf.columns)}\")\n",
    "        \n",
    "        # Check if the serial_id field exists\n",
    "        if serial_id not in gdf.columns:\n",
    "            raise ValueError(f\"Field '{serial_id}' not found in drainage areas layer. Available fields: {list(gdf.columns)}\")\n",
    "        \n",
    "        # Create dictionary with station IDs as keys and geometries as values\n",
    "        for idx, row in gdf.iterrows():\n",
    "            if row[sql_field] != 'Yes':\n",
    "                continue\n",
    "            station_id = str(row[serial_id])  # Convert to string to ensure consistency\n",
    "            station_polygons[station_id] = row['geometry']\n",
    "        \n",
    "        print(f\"Loaded {len(station_polygons)} station drainage areas\")\n",
    "        \n",
    "        # Print first few station IDs for verification\n",
    "        if station_polygons:\n",
    "            sample_ids = list(station_polygons.keys())[:5]\n",
    "            print(f\"Sample station IDs: {sample_ids}\")\n",
    "        \n",
    "        return station_polygons\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading drainage areas vector layer: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4837778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raster_path(year, month, wb_var_clean, tc_vars, processing_dir, out_geotiff):\n",
    "    \"\"\"Get the appropriate raster file path based on variable type.\"\"\"\n",
    "    if wb_var_clean not in tc_vars:\n",
    "        # Use processed variable from T&M model\n",
    "        raster_path = os.path.join(processing_dir, f\"{wb_var_clean}_{year}_{month}.tif\")\n",
    "    else:\n",
    "        # Use original GeoTIFF from TerraClimate\n",
    "        raster_path = os.path.join(out_geotiff, f\"{wb_var_clean}_{year}_{month}.tif\")\n",
    "    \n",
    "    return raster_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06805c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zonal_stats_single(args):\n",
    "    \"\"\"\n",
    "    Calculate zonal statistics for a single station, year, and month combination.\n",
    "    Each polygon is processed independently to handle overlapping features.\n",
    "    \"\"\"\n",
    "    station_id, year, month, station_data, wb_var_clean, tc_vars, processing_dir, out_geotiff = args\n",
    "    \n",
    "    try:\n",
    "        # Extract geometry from station data\n",
    "        geometry = station_data['geometry']\n",
    "        \n",
    "        # Get raster path\n",
    "        raster_path = get_raster_path(year, month, wb_var_clean, tc_vars, processing_dir, out_geotiff)\n",
    "        \n",
    "        # Check if raster file exists\n",
    "        if not os.path.exists(raster_path):\n",
    "            print(f\"Warning: Raster file not found: {raster_path}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate zonal statistics for this individual polygon\n",
    "        # Use a list with single geometry to ensure independent processing\n",
    "        stats = zonal_stats(\n",
    "            [geometry],  # Process as individual geometry\n",
    "            raster_path, \n",
    "            stats=['mean', 'count'],\n",
    "            geojson_out=False,\n",
    "            nodata=np.nan,\n",
    "            all_touched=False  # Only pixels with center inside polygon\n",
    "        )\n",
    "        \n",
    "        if stats and len(stats) > 0:\n",
    "            stat = stats[0]\n",
    "            if stat['mean'] is not None and stat['count'] is not None:\n",
    "                return {\n",
    "                    serial_id: int(station_id),\n",
    "                    'YEAR': year,\n",
    "                    'MONTH': month,\n",
    "                    'COUNT': stat['count'],\n",
    "                    'MEAN': stat['mean']\n",
    "                }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {station_id}, year {year}, month {month}: {e}\")\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da46956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(df, year, wb_var_clean, tc_vars, processing_dir, out_geotiff):\n",
    "    \"\"\"Save results to CSV file.\"\"\"\n",
    "    print(f\"\\tSaving zonal statistics results for year {year}...\")\n",
    "    \n",
    "    if wb_var_clean not in tc_vars:\n",
    "        output_path = os.path.join(processing_dir, f\"{wb_var_clean}_zonal_statistics_{year}.csv\")\n",
    "    else:\n",
    "        output_path = os.path.join(out_geotiff, f\"{wb_var_clean}_zonal_statistics_{year}.csv\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\tSaved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0700e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year_batch(year_batch, station_polygons, wb_var_clean, tc_vars, processing_dir, out_geotiff, n_processes):\n",
    "    \"\"\"\n",
    "    Process a batch of years with parallelization.\n",
    "    Each polygon is processed independently to handle overlapping features.\n",
    "    \"\"\"\n",
    "    #all_results = []\n",
    "    \n",
    "    for year in year_batch:\n",
    "        print(f\"\\n**Processing year {year}**\")\n",
    "        \n",
    "        # Prepare arguments for parallel processing\n",
    "        args_list = []\n",
    "        station_ids = list(station_polygons.keys())\n",
    "        \n",
    "        # Process each station independently\n",
    "        for station_id in station_ids:\n",
    "            station_data = station_polygons[station_id]\n",
    "            for month in months:\n",
    "                args_list.append((\n",
    "                    station_id, year, month, station_data, \n",
    "                    wb_var_clean, tc_vars, processing_dir, out_geotiff\n",
    "                ))\n",
    "        \n",
    "        print(f\"\\tCalculating zonal statistics for {len(station_ids)} stations and {len(months)} months...\")\n",
    "        print(f\"\\tTotal operations: {len(args_list)}\")\n",
    "        print(f\"\\tUsing {n_processes} processes...\")\n",
    "        print(f\"\\tNote: Each polygon processed independently to handle overlaps\")\n",
    "        \n",
    "        # Process in parallel\n",
    "        with Pool(processes=n_processes) as pool:\n",
    "            results = pool.map(calculate_zonal_stats_single, args_list)\n",
    "        \n",
    "        # Filter out None results and convert to DataFrame\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        if valid_results:\n",
    "            df_year = pd.DataFrame(valid_results)\n",
    "            #all_results.append(df_year)\n",
    "            \n",
    "            print(f\"\\tProcessed {len(valid_results)} valid records for year {year}\")\n",
    "            \n",
    "            # Save results for this year\n",
    "            save_results(df_year, year, wb_var_clean, tc_vars, processing_dir, out_geotiff)\n",
    "        else:\n",
    "            print(f\"\\tNo valid results for year {year}\")\n",
    "    \n",
    "    #return pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12c516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "    Process a batch of years with parallelization.\n",
    "    Each polygon is processed independently to handle overlapping features.\n",
    "\n",
    "    \"\"\"\n",
    "    print('############################################################')\n",
    "    print('\\t\\tINITIAL VARIABLES')\n",
    "    print(f'\\tPeriod to be executed: {years[0]}-{years[-1]}')\n",
    "    print(f'\\tVariable: {wb_var_clean}')\n",
    "    print(f'\\tProcesses: {n_processes}')\n",
    "    print('\\tNote: Each polygon processed independently to handle overlaps')\n",
    "    print('############################################################')\n",
    "    \n",
    "    # Load station drainage areas\n",
    "    station_polygons = load_station_polygons()\n",
    "    \n",
    "    if not station_polygons:\n",
    "        print(\"No station polygons loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Process years in batches to manage memory\n",
    "    batch_size = 5  # Process 5 years at a time\n",
    "    year_batches = [list(years)[i:i+batch_size] for i in range(0, len(years), batch_size)]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(years)} years in {len(year_batches)} batches...\")\n",
    "    print(f\"Each of the {len(station_polygons)} polygons will be processed independently...\")\n",
    "    \n",
    "    for i, year_batch in enumerate(year_batches, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing batch {i}/{len(year_batches)}: {year_batch}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            process_year_batch(\n",
    "                year_batch, station_polygons, wb_var_clean, \n",
    "                tc_vars, processing_dir, out_geotiff, n_processes\n",
    "            )\n",
    "            print(f\"Completed batch {i}/{len(year_batches)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nDONE!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6741e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "\t\tINITIAL VARIABLES\n",
      "\tPeriod to be executed: 1970-2023\n",
      "\tVariable: wyield\n",
      "\tProcesses: 7\n",
      "\tNote: Each polygon processed independently to handle overlaps\n",
      "############################################################\n",
      "Loading station drainage areas from vector layer...\n",
      "Loading from: Z:\\PhD_Datasets&Analysis\\Info_Inputs\\Streamflow_Sts_Drainage_Areas\\GRDC_Watersheds\\CSS-WATERSHEDS-MERGE_FINAL_SELECTION.shp\n",
      "Loaded vector layer with 809 features\n",
      "Available columns: ['grdc_no', 'river', 'station', 'area', 'altitude', 'lat_org', 'long_org', 'lat_pp', 'long_pp', 'dist_km', 'area_calc', 'quality', 'type', 'comment', 'source', 'grdcno_int', 'GRDCCOUNTR', 'Continent', 'has_monthl', 'has_daily_', 'monthly_k_', 'daily_k_re', 'Next_Downs', 'CATCHMENT_', 'Priority', 'geometry']\n",
      "Loaded 808 station drainage areas\n",
      "Sample station IDs: ['3617110', '3617811', '3617812', '3617814', '3618051']\n",
      "\n",
      "Processing 54 years in 11 batches...\n",
      "Each of the 808 polygons will be processed independently...\n",
      "\n",
      "============================================================\n",
      "Processing batch 1/11: [1970, 1971, 1972, 1973, 1974]\n",
      "============================================================\n",
      "\n",
      "**Processing year 1970**\n",
      "\tCalculating zonal statistics for 808 stations and 12 months...\n",
      "\tTotal operations: 9696\n",
      "\tUsing 7 processes...\n",
      "\tNote: Each polygon processed independently to handle overlaps\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Process all polygons\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgispro-py3-clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
