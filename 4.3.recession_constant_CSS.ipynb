{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not installed, run this cell. It is needed for the determination of the recession constant\n",
    "#!pip install hydrosignatures\n",
    "#!pip show hydrosignatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygeohydro\n",
    "#!pip show pygeohydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two lines had to be executed to correct the modifications on library dependencies that the above libraries carried out when installing\n",
    "#!pip install -U --force-reinstall scipy\n",
    "#!pip install matplotlib --upgrade\n",
    "#!pip install aiohttp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import hydrosignatures as hs\n",
    "from pygeohydro import NWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "''' Example of how to use the baseflow recession function from the hydrosignatures library '''\n",
    "\n",
    "# Get streamflow data\n",
    "nwis = NWIS()\n",
    "q = nwis.get_streamflow(\"12304500\", (\"2000-01-01\", \"2019-12-31\"))\n",
    "mrc_np, bfr_k_np = hs.baseflow_recession(q, fit_method=\"nonparametric_analytic\")\n",
    "mrc_exp, bfr_k_exp = hs.baseflow_recession(q, fit_method=\"exponential\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), dpi=100)\n",
    "\n",
    "ax1.plot(mrc_np[:, 0], mrc_np[:, 1], \"bx\")\n",
    "ax1.plot(\n",
    "    np.sort(mrc_np[:, 0]),\n",
    "    np.exp(np.log(mrc_np[0, 1]) - bfr_k_np * np.sort(mrc_np[:, 0])),\n",
    "    \"g-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax1.set_xlabel(\"Relative time [days]\")\n",
    "ax1.set_ylabel(\"Flow [cfs]\")\n",
    "ax1.set_xlim(0, 80)\n",
    "ax1.set_ylim(0, 80)\n",
    "ax1.set_title(\"Non-parametric fitted recession curve\")\n",
    "ax1.legend([\"MRC\", f\"Exponential fit (K={bfr_k_np:.4f})\"])\n",
    "\n",
    "ax2.plot(mrc_exp[:, 0], mrc_exp[:, 1], \"bx\")\n",
    "ax2.plot(\n",
    "    np.sort(mrc_exp[:, 0]),\n",
    "    np.exp(np.log(mrc_exp[0, 1]) - bfr_k_exp * np.sort(mrc_exp[:, 0])),\n",
    "    \"g-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.set_xlabel(\"Relative time [days]\")\n",
    "ax2.set_ylabel(\"Flow [cfs]\")\n",
    "ax2.set_xlim(0, 80)\n",
    "ax2.set_ylim(0, 80)\n",
    "ax2.set_title(\"Exponential fitted recession curve\")\n",
    "ax2.legend([\"MRC\", f\"Exponential fit (K={bfr_k_exp:.4f})\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "''' Comparion of the alpha value obtained from the example (without weights) with the alpha value obtained by using the 'polyfit' function from numpy (with weights) '''\n",
    "\n",
    "x = list(mrc_np[:, 0])\n",
    "y = list(mrc_np[:, 1])\n",
    "\n",
    "# Fit a linear regression model\n",
    "coefficients = np.polyfit(x, np.log(y), 1, w=np.sqrt(y)) # Use of weights (w) to avoid bias towards small values\n",
    "beta = np.exp(coefficients[1])\n",
    "alpha = coefficients[0]\n",
    "print(\"The alpha value differs from the example ('bfr_k_np') when a weight is assigned in the 'polyfit' function:\", round(alpha, 4))\n",
    "\n",
    "# Fit a linear regression model\n",
    "coefficients2 = np.polyfit(x, np.log(y), 1) # Use of weights (w) to avoid bias towards small values\n",
    "beta2 = np.exp(coefficients2[1])\n",
    "alpha2 = coefficients2[0]\n",
    "print(\"The alpha value is the same value that that from the example ('bfr_k_np') when a weight is not assigned in the 'polyfit' function:\", round(alpha2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "''' Plot of the non-parametric fitted recession curve with and without weights '''\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), dpi=100)\n",
    "\n",
    "ax1.plot(mrc_np[:, 0], mrc_np[:, 1], \"bx\")\n",
    "ax1.plot(\n",
    "    np.sort(mrc_np[:, 0]),\n",
    "    np.exp(np.log(mrc_np[0, 1]) - abs(alpha2) * np.sort(mrc_np[:, 0])),\n",
    "    \"g-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax1.set_xlabel(\"Relative time [days]\")\n",
    "ax1.set_ylabel(\"Flow [cfs]\")\n",
    "ax1.set_xlim(0, 80)\n",
    "ax1.set_ylim(0, 80)\n",
    "ax1.set_title(\"Non-parametric fitted recession curve\\nWithout weights\")\n",
    "ax1.legend([\"MRC\", f\"Exponential fit (K={abs(alpha2):.4f})\"])\n",
    "\n",
    "ax2.plot(mrc_np[:, 0], mrc_np[:, 1], \"bx\")\n",
    "ax2.plot(\n",
    "    np.sort(mrc_np[:, 0]),\n",
    "    np.exp(np.log(mrc_np[0, 1]) - abs(alpha) * np.sort(mrc_np[:, 0])),\n",
    "    \"g-\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax2.set_xlabel(\"Relative time [days]\")\n",
    "ax2.set_ylabel(\"Flow [cfs]\")\n",
    "ax2.set_xlim(0, 80)\n",
    "ax2.set_ylim(0, 80)\n",
    "ax2.set_title(\"Non-parametric fitted recession curve\\n With weights\")\n",
    "ax2.legend([\"MRC\", f\"Exponential fit (K={abs(alpha):.4f})\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coeficiente de determinación ($R^2$)**  \n",
    "   - Se obtiene elevando al cuadrado el coeficiente de correlación ($R^2$), lo que significa que **no distingue entre correlaciones positivas y negativas**.  \n",
    "   - Representa el **porcentaje de variabilidad** de una variable que es explicada por la otra en un modelo de regresión.  \n",
    "   - Si quieres evaluar **qué tan bien un modelo explica la variabilidad de los datos**, este es el mejor indicador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "''' Coefficient of determination of the non-parametric fitted recession curve with and without weights '''\n",
    "\n",
    "corr_matrix = np.corrcoef(mrc_np[:, 1], np.exp(np.log(mrc_np[0, 1]) - abs(alpha2) * np.sort(mrc_np[:, 0])))\n",
    "corr = corr_matrix[0,1]\n",
    "R_sq = corr**2\n",
    "print(\"Coeffcient of determination with alpha value\", abs(round(alpha2, 4)), \"--without weights-- is\", round(R_sq, 3))\n",
    "\n",
    "corr_matrix2 = np.corrcoef(mrc_np[:, 1], np.exp(np.log(mrc_np[0, 1]) - abs(alpha) * np.sort(mrc_np[:, 0])))\n",
    "corr2 = corr_matrix2[0,1]\n",
    "R_sq2 = corr2**2\n",
    "print(\"Coeffcient of determination with alpha value\", abs(round(alpha, 4)), \"--with weights-- is\", round(R_sq2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole-period daily dataset\n",
    "daily_folder = r\"Z:\\PhD_Datasets&Analysis\\Info_Inputs\\Streamflow_Stations\\Climate_Sensitive_Stations-GRDC\\2025-02-13_17-18_Daily\"\n",
    "\n",
    "# All daily continuos data folder\n",
    "lcd_folder = daily_folder + \"//Baseflow//longest_continuous_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DF with the longest continuos periods for all the stations\n",
    "longest_periods_df = pd.read_csv(lcd_folder + \"//longest_continuos_df.csv\")\n",
    "longest_periods_df['Station'] = longest_periods_df['Station'].astype(str)\n",
    "longest_periods_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the DF with all the daily data\n",
    "df_cleaned = pd.read_csv(daily_folder + \"\\_DataFrames\\Joined_Daily_Sts_DFs.csv\", index_col=\"YYYY-MM-DD\")\n",
    "df_cleaned.index = pd.to_datetime(df_cleaned.index, format='%Y-%m-%d')\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"hs.baseflow_recession()\"**: Calculate baseflow recession constant and master recession curve.\n",
    "\n",
    "*Notes*\\\n",
    "This function is ported from the TOSSH Matlab toolbox, which is based on the following publication:\n",
    "\n",
    "- Gnann, S.J., Coxon, G., Woods, R.A., Howden, N.J.K., McMillan H.K., 2021. TOSSH: A Toolbox for Streamflow Signatures in Hydrology. Environmental Modelling & Software. https://doi.org/10.1016/j.envsoft.2021.104983\n",
    "\n",
    "This function calculates baseflow recession constant assuming exponential recession behaviour (Safeeq et al., 2013). Master recession curve (MRC) is constructed using the adapted matching strip method (Posavec et al., 2006).\n",
    "\n",
    "According to Safeeq et al. (2013), K < 0.065 represent groundwater dominated slow-draining systems, K >= 0.065 represent shallow subsurface flow dominated fast draining systems.\n",
    "\n",
    "*Parameters*\n",
    "- streamflow : numpy.ndarray\\\n",
    "    Streamflow as a 1D array.\n",
    "\n",
    "- freq : float, optional\\\n",
    "    Frequency of steamflow in number of days. Default is 1, i.e., daily streamflow.\n",
    "\n",
    "- recession_length : int, optional\\\n",
    "    Minimum length of recessions [days]. Default is 15.\n",
    "\n",
    "- n_start : int, optional\\\n",
    "    Days to be removed after start of recession. Default is 0.\n",
    "\n",
    "- eps : float, optional\\\n",
    "    Allowed increase in flow during recession period. Default is 0.\n",
    "\n",
    "- start_of_recession : {'baseflow', 'peak'}, optional\\\n",
    "    Define start of recession. Default is 'baseflow'.\n",
    "\n",
    "- fit_method : {'nonparametric_analytic', 'exponential'}, optional\\\n",
    "    Method to fit mrc. Default is 'nonparametric_analytic'.\n",
    "\n",
    "- lyne_hollick_smoothing : float, optional\\\n",
    "    Smoothing parameter of Lyne-Hollick filter. Default is 0.925.\n",
    "\n",
    "*Returns*\n",
    "- mrc : numpy.ndarray\\\n",
    "    Master Recession Curve as 2D array of [time, flow].\n",
    "\n",
    "* bf_recession_k : float\\\n",
    "    Baseflow Recession Constant [1/day].\n",
    "\n",
    "*Raises*\n",
    "- ValueError\\\n",
    "    If no recession segments are found or if a complex BaseflowRecessionK is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Determination of alpha factor with all the daily data series '''\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "for station in df_cleaned.columns:\n",
    "    print(\"Processing station:\", station)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to calculate alpha without weights\n",
    "        mrc_np, alpha_without_weights = hs.baseflow_recession(df_cleaned[station], fit_method=\"nonparametric_analytic\")\n",
    "        \n",
    "        # Extract x and y values\n",
    "        x = list(mrc_np[:, 0])\n",
    "        y = list(mrc_np[:, 1])\n",
    "\n",
    "        # Calculate alpha with weights\n",
    "        coefficients = np.polyfit(x, np.log(y), 1, w=np.sqrt(y))\n",
    "        alpha_with_weights = abs(coefficients[0])\n",
    "\n",
    "        # Compute R^2 without weights\n",
    "        corr_matrix_without_weights = np.corrcoef(y, np.exp(np.log(mrc_np[0, 1]) - alpha_without_weights * np.sort(x)))\n",
    "        corr_without_weights = corr_matrix_without_weights[0, 1]\n",
    "        R_sq_without_weights = corr_without_weights ** 2\n",
    "\n",
    "        # Compute R^2 with weights\n",
    "        corr_matrix_with_weights = np.corrcoef(y, np.exp(np.log(mrc_np[0, 1]) - alpha_with_weights * np.sort(x)))\n",
    "        corr_with_weights = corr_matrix_with_weights[0, 1]\n",
    "        R_sq_with_weights = corr_with_weights ** 2\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {station}: {e}\")\n",
    "        \n",
    "        # Assign NaN values if an error occurs\n",
    "        alpha_without_weights = np.nan\n",
    "        R_sq_without_weights = np.nan\n",
    "        alpha_with_weights = np.nan\n",
    "        R_sq_with_weights = np.nan\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results.append({\n",
    "        \"station\": station,\n",
    "        \"alpha_without_weights\": alpha_without_weights,\n",
    "        \"R_sq_without_weights\": R_sq_without_weights,\n",
    "        \"alpha_with_weights\": alpha_with_weights,\n",
    "        \"R_sq_with_weights\": R_sq_with_weights\n",
    "    })\n",
    "\n",
    "# Convert list of dictionaries into a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Determination of alpha factor with only the longest continuous daily data periods '''\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results2 = []\n",
    "\n",
    "for station in df_cleaned.columns:\n",
    "    print(\"Processing station:\", station)\n",
    "\n",
    "    # Get the start and end dates of the longest period of each station\n",
    "    start_date = longest_periods_df[longest_periods_df[\"Station\"] == station][\"StartDate\"].to_list()[0]\n",
    "    end_date = longest_periods_df[longest_periods_df[\"Station\"] == station][\"EndDate\"].to_list()[0]\n",
    "    \n",
    "    try:\n",
    "        # Attempt to calculate alpha without weights\n",
    "        mrc_np, alpha_without_weights = hs.baseflow_recession(df_cleaned.loc[start_date:end_date, [station]], fit_method=\"nonparametric_analytic\")\n",
    "        \n",
    "        # Extract x and y values\n",
    "        x = list(mrc_np[:, 0])\n",
    "        y = list(mrc_np[:, 1])\n",
    "\n",
    "        # Calculate alpha with weights\n",
    "        coefficients = np.polyfit(x, np.log(y), 1, w=np.sqrt(y))\n",
    "        alpha_with_weights = abs(coefficients[0])\n",
    "\n",
    "        # Compute R^2 without weights\n",
    "        corr_matrix_without_weights = np.corrcoef(y, np.exp(np.log(mrc_np[0, 1]) - alpha_without_weights * np.sort(x)))\n",
    "        corr_without_weights = corr_matrix_without_weights[0, 1]\n",
    "        R_sq_without_weights = corr_without_weights ** 2\n",
    "\n",
    "        # Compute R^2 with weights\n",
    "        corr_matrix_with_weights = np.corrcoef(y, np.exp(np.log(mrc_np[0, 1]) - alpha_with_weights * np.sort(x)))\n",
    "        corr_with_weights = corr_matrix_with_weights[0, 1]\n",
    "        R_sq_with_weights = corr_with_weights ** 2\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {station}: {e}\")\n",
    "        \n",
    "        # Assign NaN values if an error occurs\n",
    "        alpha_without_weights = np.nan\n",
    "        R_sq_without_weights = np.nan\n",
    "        alpha_with_weights = np.nan\n",
    "        R_sq_with_weights = np.nan\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results2.append({\n",
    "        \"station\": station,\n",
    "        \"alpha_without_weights\": alpha_without_weights,\n",
    "        \"R_sq_without_weights\": R_sq_without_weights,\n",
    "        \"alpha_with_weights\": alpha_with_weights,\n",
    "        \"R_sq_with_weights\": R_sq_with_weights\n",
    "    })\n",
    "\n",
    "# Convert list of dictionaries into a DataFrame\n",
    "df_results2 = pd.DataFrame(results2)\n",
    "df_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series1 = pd.Series(df_results.describe()[\"R_sq_with_weights\"], name=\"ALL_DATA\") # All daily data series\n",
    "series2 = pd.Series(df_results2.describe()[\"R_sq_with_weights\"], name=\"CONT_DATA\") # Only the longest continuous daily data periods\n",
    "# Concatenate the series into a DataFrame for comparison of the statistics of R2 \n",
    "R_sq_mean = pd.concat([series1, series2], axis=1)\n",
    "R_sq_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of the **R² with weights** (`R_sq_with_weights`) values for the two datasets:  \n",
    "\n",
    "- **ALL_DATA**: Statistics resulting from data that includes NaN values from missing data.  \n",
    "- **CONT_DATA**: Statistics resulting from data that contains only the longest continuous period for each station.  \n",
    "\n",
    "### Observations from the Comparison:\n",
    "1. **Count Difference**:  \n",
    "   - ALL_DATA has **794** valid values, while CONT_DATA has **788**, suggesting that for a bigger number of stations the baseflow alpha factor could not be determined when using the longest continuous period approach.  \n",
    "\n",
    "2. **Mean & Standard Deviation**:  \n",
    "   - The mean R² is slightly higher for **ALL_DATA (0.946)** than **CONT_DATA (0.943)**.  \n",
    "   - The standard deviation is slightly lower for CONT_DATA, suggesting slightly more **consistency** in R² values when using continuous data.  \n",
    "\n",
    "3. **Minimum Value**:  \n",
    "   - The lowest R² in **ALL_DATA** is **0.2118**, much lower than the minimum in **CONT_DATA (0.5211)**.  \n",
    "   - This suggests that **including no-data periods might have led to some very poor model fits** (possibly due to discontinuities in the hydrograph).  \n",
    "\n",
    "4. **Median (50%) & Quartiles**:  \n",
    "   - The **median** is nearly identical (**0.9663 for both**), suggesting that for most stations, the effect of using continuous data is minimal.  \n",
    "   - The **interquartile range (IQR)** is also similar, indicating that most R² values are concentrated around high values (**above 0.92**).  \n",
    "\n",
    "5. **Maximum Value**:  \n",
    "   - The max R² values are nearly identical (**0.9989 vs. 0.9993**), meaning that for the best-fit cases, the difference in data selection method has no significant effect.  \n",
    "\n",
    "### Interpretation:\n",
    "- **Removing discontinuous periods seems to slightly reduce variability** (lower standard deviation).  \n",
    "- **Extremely low R² values disappear** in the continuous dataset, suggesting that missing data might be **hurting model performance** for some stations.  \n",
    "- **For the majority of stations**, there’s no significant difference in R², meaning that the **baseflow recession analysis is robust** whether or not small gaps in data exist.  \n",
    "\n",
    "### Next Steps:\n",
    "- You could **investigate the stations where R² dropped significantly** (e.g., check which stations had an R² < 0.5 in the ALL_DATA case but improved in CONT_DATA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#### Step 1: Check Normality of Differences ####\n",
    "# First, test if the differences are normally distributed using the Shapiro-Wilk test.\n",
    "# If p > 0.05, the differences follow a normal distribution, and you can use a paired t-test.\n",
    "# If p ≤ 0.05, the differences do not follow a normal distribution, and you should use the Wilcoxon signed-rank test instead.\n",
    "\n",
    "# Compute the differences\n",
    "differences = df_results[\"R_sq_with_weights\"] - df_results2[\"R_sq_with_weights\"]\n",
    "differences = differences.dropna()  # Remove NaN values\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_test = stats.shapiro(differences)\n",
    "print(\"Shapiro-Wilk test p-value:\", shapiro_test.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#### Step 2: Perform Statistical Test ####\n",
    "\n",
    "# Drop NaN values before performing the test\n",
    "valid_data = df_results[\"R_sq_with_weights\"].dropna()\n",
    "valid_cont_data = df_results2[\"R_sq_with_weights\"].dropna()\n",
    "\n",
    "# Ensure both series have the same length\n",
    "valid_data, valid_cont_data = valid_data.align(valid_cont_data, join='inner')\n",
    "\n",
    "# A) If Differences Are Normally Distributed → Use Paired t-test\n",
    "#t_stat, p_value = stats.ttest_rel(valid_data, valid_cont_data)\n",
    "#print(\"Paired t-test p-value:\", p_value)\n",
    "\n",
    "# B) If Differences Are Not Normally Distributed → Use Wilcoxon Signed-Rank Test\n",
    "wilcoxon_test = stats.wilcoxon(valid_data, valid_cont_data)\n",
    "print(\"Wilcoxon test p-value:\", wilcoxon_test.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **If p ≤ 0.05**, the difference is statistically significant.\n",
    "- **If p > 0.05**, there is no significant difference.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpreting the Results**\n",
    "- If **both tests give p > 0.05**, there is **no significant difference** in R² values between the two datasets.\n",
    "- If **p ≤ 0.05**, the difference is **statistically significant**, meaning using the longest continuous period **impacts the R² values**.\n",
    "\n",
    "**CONCLUSION:** Based on the analysis, it is better to use the results obtained with the longest continuous data periods (`df_results2`). One key point is that by considering these results, some very poor model fits are avoided and the selected alpha values are those that were determined from model fits with $R^2$ greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_recessions = df_results2[[\"station\", \"alpha_with_weights\"]].copy()\n",
    "k_recessions[\"k_recession\"] = k_recessions[\"alpha_with_weights\"].apply(lambda row: np.exp(-row))\n",
    "k_recessions = k_recessions.dropna().reset_index(drop=True)\n",
    "k_recessions.to_csv(lcd_folder + \"//k_recessions_df.csv\", index=False) # Save file for possible further analysis\n",
    "k_recessions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
